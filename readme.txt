A simple structure to train a network.
@Author: Fan
*********Active function
Sigmoid
ReLU
LeakyReLU
tanh
Softmax
*********

*********Loss function
CrossEntrop
L2 distance
L1 distance
*********

*********Optimizer strages
SGD
Adam
Momentum
*********

*********initialize strages
Gaussion initialize
Constant initialize
Xavier intialize
he_normal initialize
*********

*********regularization strages
BatchNormal
Dropout
parameters regular
*********

*********CNN support
padding
pooling
kernal
*********

*********RNN support
simple RNN
LSTM
*********



